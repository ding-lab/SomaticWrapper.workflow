This is prior work developing Docker workflow at MGI.  See below for
* description of MGI Docker evironment
* how to launch jobs
* analysis of results from docker and master branches, showing things work

Scripts in this directory are superceded by those in ../src

-----

MGI environment is unique and requires different procedures.  Specifically,
* Docker environment has MGI volumes mounted
    * Both Docker and MGI data available
        * Container data in e.g. /data not visible from outside container
        * Container still sees user home directory and other MGI partitions, e.g. /gscmnt/gc3025/dinglab
        * Take care to avoid MGI-specific paths with an image
    * Path issues arise when MGI user configuration files (e.g. .bashrc) are automatically evaluated.
        * We install in image a configuration file /home/bps/mgi-bps.bashrc which sets paths.  It is evaluated upon
          startup by script /home/bps/mgi-bps_start.sh
* User is same uid as at MGI, and cannot run anything as root
    * As a consequence, files installed as part of Dockerfile (by 'root') cannot be edited from within
      container (no write privileges)
    * It is more convenient during development to use MGI user directory than e.g. /usr/local/somaticwrapper,
      since the latter can be modified while the former cannot
* There is no `docker` command.  Docker container is launched using bsub
    * Cannot build an image at MGI
    * Cannot exec into a running container
    * Will execute on an arbitrary machine
        * Container data in e.g. /data is lost when job exits unless shared volume mounted during initialization


## Preliminary steps

This has to be done on any new installation.  Running Strelka Demo is a special case.

TODO: Describe how to set up image data for arbitrary run, Strelka Demo

## Executing within the container

Use `bash bash_SomaticWrapper_MGI.sh` to launch the container.  
This will launch on a random machine (will take time to download image).

You can check if in container if /data exists (there are better ways).  Do:

export CONFIG=/data/run_config/01BR001.config

Then run the steps by executing each individually (e.g., `bash 1_run_strelka.sh`)


## Executing as a job on the cluster

# See notes in /gscuser/mwyczalk/projects/SomaticWrapper/SW_testing

BSUB output directory: /gscuser/mwyczalk/projects/SomaticWrapper/runtime_bsub

To run step 2 in container, run the following on host machine,

bash submit_SomaticWrapper_MGI.sh 2 /data/run_config/01BR001.config

Note, Pindel requires increased memory to keep from crashing.  TODO: test automatically to see if pindel succeeds (exit status?)

-------

Critical comparison of original vs. new 01BR001 run

Original:  /gscuser/scao/gc2521/dinglab/cptac_prospective_samples/exome/somatic/test.for.matt/01BR001
New: /data/data/01BR001 aka /gscmnt/gc2521/dinglab/mwyczalk/somatic-wrapper-data/data/01BR001

Key files:
* strelka.somatic.snv.all.gvip.dbsnp_pass.vcf
  - OLD - 699 non-header entries in VCF
  - NEW - 1095 non-header entries in VCF
* varscan.out.som_snv.gvip.Somatic.hc.somfilter_pass.dbsnp_pass.vcf
  - old - 363 entries
  - new - 603 entries
* pindel.out.current_final.gvip.dbsnp_pass.vcf
  - old - 285 entries
  - new - 895 entries  
* varscan.out.som_indel.gvip.Somatic.hc.dbsnp_pass.vcf
  - old - 100 entries
  - new - 1927 entries
* merged.vcf
  - old - 1429 entries
  - new - 3754 entries

*** need to test exit status to make sure jobs completed successfully

---
Example of running arbitrary command in container:

bash submit_adhoc.sh "cd /gscuser/mwyczalk/projects/SomaticWrapper/somaticwrapper/image.setup/B_Filter; /bin/bash ./3b_make_variant_filter_GRCh38.sh"

NOTE: this is very ad hoc, and needs further work.  In particular, runtime and log directories should go in data directory
